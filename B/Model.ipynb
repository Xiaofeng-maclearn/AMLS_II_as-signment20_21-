{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this python, we input the id-list data into an RNN and a LSTM model, train the model and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras as kr\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               text_code      sent  \\\n",
      "0      [    3     1     0     2  5717    31   108 108...  negative   \n",
      "1      [    3     0   332     0    38     2   934   2...  negative   \n",
      "2      [   1    0    2    7  315    0    0 1144    0 ...  negative   \n",
      "3      [    1     0     0  1373  4294   253    41    ...  negative   \n",
      "4      [    0     1    10    17   712     0  2404    ...  negative   \n",
      "...                                                  ...       ...   \n",
      "10546  [  11    0    0    0  534    0    0  263    0 ...  positive   \n",
      "10547  [  12    1  550    0   22   46 6911  121   14 ...  positive   \n",
      "10548  [   0   61    0   92  121   14    0  133 6174 ...  positive   \n",
      "10549  [    0     9    10   335     0     0   183   2...  positive   \n",
      "10550  [   0    1    0  638    0    0  177  247    9 ...  positive   \n",
      "\n",
      "             topic  \n",
      "0      amy schumer  \n",
      "1      amy schumer  \n",
      "2      amy schumer  \n",
      "3      amy schumer  \n",
      "4      amy schumer  \n",
      "...            ...  \n",
      "10546         zayn  \n",
      "10547         zayn  \n",
      "10548         zayn  \n",
      "10549         zayn  \n",
      "10550         zayn  \n",
      "\n",
      "[10551 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# open and read the id-list\n",
    "df = pd.read_csv('code.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the id-list is a currently in String type, we have to change it into a numeric array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type-change function\n",
    "def textcode_to_array(textcode):\n",
    "    listOfTokens = re.split(r'\\W+',textcode)\n",
    "    # decrease the length of array since there are spaces at the start and end of the splitted list\n",
    "    codes = np.zeros(len(listOfTokens)-2, dtype=np.int) \n",
    "    for i in range(len(codes)):\n",
    "        codes[i] = int(listOfTokens[i+1])\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               text_code      sent  \\\n",
      "0      [    3     1     0     2  5717    31   108 108...  negative   \n",
      "1      [    3     0   332     0    38     2   934   2...  negative   \n",
      "2      [   1    0    2    7  315    0    0 1144    0 ...  negative   \n",
      "3      [    1     0     0  1373  4294   253    41    ...  negative   \n",
      "4      [    0     1    10    17   712     0  2404    ...  negative   \n",
      "...                                                  ...       ...   \n",
      "10546  [  11    0    0    0  534    0    0  263    0 ...  positive   \n",
      "10547  [  12    1  550    0   22   46 6911  121   14 ...  positive   \n",
      "10548  [   0   61    0   92  121   14    0  133 6174 ...  positive   \n",
      "10549  [    0     9    10   335     0     0   183   2...  positive   \n",
      "10550  [   0    1    0  638    0    0  177  247    9 ...  positive   \n",
      "\n",
      "             topic                                               code  \n",
      "0      amy schumer  [3, 1, 0, 2, 5717, 31, 108, 10841, 1585, 10840...  \n",
      "1      amy schumer  [3, 0, 332, 0, 38, 2, 934, 280, 10838, 0, 0, 1...  \n",
      "2      amy schumer  [1, 0, 2, 7, 315, 0, 0, 1144, 0, 242, 125, 330...  \n",
      "3      amy schumer  [1, 0, 0, 1373, 4294, 253, 41, 0, 2, 363, 798,...  \n",
      "4      amy schumer  [0, 1, 10, 17, 712, 0, 2404, 0, 16, 14, 583, 1...  \n",
      "...            ...                                                ...  \n",
      "10546         zayn  [11, 0, 0, 0, 534, 0, 0, 263, 0, 1, 0, 6910, 0...  \n",
      "10547         zayn  [12, 1, 550, 0, 22, 46, 6911, 121, 14, 448, 0,...  \n",
      "10548         zayn  [0, 61, 0, 92, 121, 14, 0, 133, 6174, 0, 10, 0...  \n",
      "10549         zayn  [0, 9, 10, 335, 0, 0, 183, 284, 16, 0, 13464, ...  \n",
      "10550         zayn  [0, 1, 0, 638, 0, 0, 177, 247, 9, 22, 0, 0, 0,...  \n",
      "\n",
      "[10551 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# do the type change\n",
    "df['code'] = \"\"\n",
    "for i in range(df.shape[0]):\n",
    "    df.iloc[i,3] =  textcode_to_array(df.iloc[i,0])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2 # positive, negative\n",
    "# use labelencoder to change the type of classes from String to integer\n",
    "le = preprocessing.LabelEncoder() \n",
    "labels = le.fit_transform(df.iloc[:,1])\n",
    "# and then, change the type from integer to one-hot code\n",
    "labels = kr.utils.to_categorical(labels, num_classes) \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into train, validation and test sets in a ratio 6:2:2\n",
    "X_trainval,X_test,y_trainval,y_test=train_test_split(df.iloc[:,3],labels,test_size=0.2,random_state=1) \n",
    "X_train,X_val,y_train,y_val=train_test_split(X_trainval,y_trainval,test_size=0.25,random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "# find the max length of id-lists\n",
    "maxlen = 0;\n",
    "for i in range(10551):\n",
    "    length = len(df.iloc[i,3])\n",
    "    if length>maxlen:\n",
    "        maxlen = length\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4421     [0, 21, 250, 0, 246, 515, 1244, 0, 1, 0, 332, ...\n",
      "3675     [0, 0, 0, 0, 1, 10, 17, 530, 0, 0, 370, 124, 3...\n",
      "6668     [2, 55, 1, 95, 161, 32, 0, 346, 0, 0, 204, 63,...\n",
      "4645                      [1036, 0, 195, 27, 0, 1, 11, 28]\n",
      "6824     [0, 0, 0, 61, 0, 38, 0, 179, 201, 435, 11, 0, ...\n",
      "                               ...                        \n",
      "9279     [0, 1341, 0, 2, 397, 0, 334, 730, 0, 2, 558, 8...\n",
      "10376    [0, 189, 326, 5, 1, 111, 8, 6755, 603, 15, 52,...\n",
      "4400     [0, 3573, 130, 25, 0, 0, 1587, 2441, 8, 2976, ...\n",
      "5448     [0, 3, 59, 45, 51, 758, 3844, 101, 0, 2, 1217,...\n",
      "4152     [0, 0, 22, 0, 219, 2, 77, 35, 0, 19, 0, 132, 8...\n",
      "Name: code, Length: 2111, dtype: object\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have processed the data, now we should prepare our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 2 # number of cells in a hidden layer\n",
    "batch_size=30 # size of batch\n",
    "n_batch=211 # we have 6330 train data, so 6330/30 = 211 batches\n",
    "train_rate = 0.0001 # training rate of optimizer\n",
    "embedding_size = 32 # dimension of embedding set\n",
    "vocabulary_size = 13465 # length of dictionary\n",
    "\n",
    "# placeholders for input data and output probabilities\n",
    "x=tf.placeholder(tf.int32,shape=[None,maxlen]) #\n",
    "y=tf.placeholder(tf.float32,[None,num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases which transform the last output of RNN model into a num_classes dimensional probability vector\n",
    "weights=tf.Variable(tf.truncated_normal([num_units,num_classes],stddev=0.1))\n",
    "bias=tf.Variable(tf.constant(0.1,shape=[num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an embedding matrix for the dictionary\n",
    "embedding = tf.get_variable('embedding', [vocabulary_size, embedding_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probability for dropout process\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef RNN(x,weights,bias):\\n    embedding_inputs = tf.nn.embedding_lookup(embedding, x) # replace the integers in id list into embedding vectors\\n    basic_cell=tf.nn.rnn_cell.BasicRNNCell(num_units) # generate a basic RNN model with num_units cells in each layer\\n    \\n    outputs, states = tf.nn.dynamic_rnn(basic_cell,embedding_inputs, dtype=tf.float32) # run the model and get the last output\\n    outputs = tf.nn.dropout(outputs, keep_prob) # dropout process\\n    result = tf.nn.softmax(tf.matmul(outputs[:,-1,:],weights)+bias,1) # calculate the probability vector\\n    return result\\n\\nprediction=RNN(x,weights,bias) # get the probability vector\\ncost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y)) # calculate the cost\\ntrain_step=tf.train.AdamOptimizer(train_rate).minimize(cost) # use optimizer to improve the model in a given rate\\ncorrect_predict=tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) # generate accuracy vector\\naccuracy=tf.reduce_mean(tf.cast(correct_predict,tf.float32)) # calculate accuracy\\n\\ninit=tf.global_variables_initializer() # initialize all variables\\n\\n# run the training function\\nwith tf.Session() as sess:\\n    sess.run(init)\\n    for i in range(n_batch):\\n        TrainData_label=y_train[i*batch_size:(i+1)*n_batch]\\n        TrainData_batch = kr.preprocessing.sequence.pad_sequences(X_train.iloc[i*batch_size:(i+1)*n_batch], maxlen) # increase the sequence length to maxlen\\n        sess.run(train_step,feed_dict={x:TrainData_batch,y:TrainData_label,keep_prob: 0.75})\\n        print(\"batch: \")\\n        print(i)\\n    print(\"train accuracy: \")\\n    TrainData = kr.preprocessing.sequence.pad_sequences(X_train, maxlen)\\n    print(sess.run(accuracy, feed_dict={x: TrainData, y: y_train,keep_prob: 1})) # Prediction does not need dropout, so 1 is set\\n    print(\"val accuracy: \")\\n    ValData = kr.preprocessing.sequence.pad_sequences(X_val, maxlen)\\n    print(sess.run(accuracy, feed_dict={x: ValData, y: y_val,keep_prob: 1}))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple RNN model\n",
    "'''\n",
    "def RNN(x,weights,bias):\n",
    "    embedding_inputs = tf.nn.embedding_lookup(embedding, x) # replace the integers in id list into embedding vectors\n",
    "    basic_cell=tf.nn.rnn_cell.BasicRNNCell(num_units) # generate a basic RNN model with num_units cells in each layer\n",
    "    \n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell,embedding_inputs, dtype=tf.float32) # run the model and get the last output\n",
    "    outputs = tf.nn.dropout(outputs, keep_prob) # dropout process\n",
    "    result = tf.nn.softmax(tf.matmul(outputs[:,-1,:],weights)+bias,1) # calculate the probability vector\n",
    "    return result\n",
    "\n",
    "prediction=RNN(x,weights,bias) # get the probability vector\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y)) # calculate the cost\n",
    "train_step=tf.train.AdamOptimizer(train_rate).minimize(cost) # use optimizer to improve the model in a given rate\n",
    "correct_predict=tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) # generate accuracy vector\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_predict,tf.float32)) # calculate accuracy\n",
    "\n",
    "init=tf.global_variables_initializer() # initialize all variables\n",
    "\n",
    "# run the training function\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(n_batch):\n",
    "        TrainData_label=y_train[i*batch_size:(i+1)*n_batch]\n",
    "        TrainData_batch = kr.preprocessing.sequence.pad_sequences(X_train.iloc[i*batch_size:(i+1)*n_batch], maxlen) # increase the sequence length to maxlen\n",
    "        sess.run(train_step,feed_dict={x:TrainData_batch,y:TrainData_label,keep_prob: 0.75})\n",
    "        print(\"batch: \")\n",
    "        print(i)\n",
    "    print(\"train accuracy: \")\n",
    "    TrainData = kr.preprocessing.sequence.pad_sequences(X_train, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: TrainData, y: y_train,keep_prob: 1})) # Prediction does not need dropout, so 1 is set\n",
    "    print(\"val accuracy: \")\n",
    "    ValData = kr.preprocessing.sequence.pad_sequences(X_val, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: ValData, y: y_val,keep_prob: 1}))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\lab\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-14-b5b42db1f460>:13: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "batch: \n",
      "0\n",
      "batch: \n",
      "1\n",
      "batch: \n",
      "2\n",
      "batch: \n",
      "3\n",
      "batch: \n",
      "4\n",
      "batch: \n",
      "5\n",
      "batch: \n",
      "6\n",
      "batch: \n",
      "7\n",
      "batch: \n",
      "8\n",
      "batch: \n",
      "9\n",
      "batch: \n",
      "10\n",
      "batch: \n",
      "11\n",
      "batch: \n",
      "12\n",
      "batch: \n",
      "13\n",
      "batch: \n",
      "14\n",
      "batch: \n",
      "15\n",
      "batch: \n",
      "16\n",
      "batch: \n",
      "17\n",
      "batch: \n",
      "18\n",
      "batch: \n",
      "19\n",
      "batch: \n",
      "20\n",
      "batch: \n",
      "21\n",
      "batch: \n",
      "22\n",
      "batch: \n",
      "23\n",
      "batch: \n",
      "24\n",
      "batch: \n",
      "25\n",
      "batch: \n",
      "26\n",
      "batch: \n",
      "27\n",
      "batch: \n",
      "28\n",
      "batch: \n",
      "29\n",
      "batch: \n",
      "30\n",
      "batch: \n",
      "31\n",
      "batch: \n",
      "32\n",
      "batch: \n",
      "33\n",
      "batch: \n",
      "34\n",
      "batch: \n",
      "35\n",
      "batch: \n",
      "36\n",
      "batch: \n",
      "37\n",
      "batch: \n",
      "38\n",
      "batch: \n",
      "39\n",
      "batch: \n",
      "40\n",
      "batch: \n",
      "41\n",
      "batch: \n",
      "42\n",
      "batch: \n",
      "43\n",
      "batch: \n",
      "44\n",
      "batch: \n",
      "45\n",
      "batch: \n",
      "46\n",
      "batch: \n",
      "47\n",
      "batch: \n",
      "48\n",
      "batch: \n",
      "49\n",
      "batch: \n",
      "50\n",
      "batch: \n",
      "51\n",
      "batch: \n",
      "52\n",
      "batch: \n",
      "53\n",
      "batch: \n",
      "54\n",
      "batch: \n",
      "55\n",
      "batch: \n",
      "56\n",
      "batch: \n",
      "57\n",
      "batch: \n",
      "58\n",
      "batch: \n",
      "59\n",
      "batch: \n",
      "60\n",
      "batch: \n",
      "61\n",
      "batch: \n",
      "62\n",
      "batch: \n",
      "63\n",
      "batch: \n",
      "64\n",
      "batch: \n",
      "65\n",
      "batch: \n",
      "66\n",
      "batch: \n",
      "67\n",
      "batch: \n",
      "68\n",
      "batch: \n",
      "69\n",
      "batch: \n",
      "70\n",
      "batch: \n",
      "71\n",
      "batch: \n",
      "72\n",
      "batch: \n",
      "73\n",
      "batch: \n",
      "74\n",
      "batch: \n",
      "75\n",
      "batch: \n",
      "76\n",
      "batch: \n",
      "77\n",
      "batch: \n",
      "78\n",
      "batch: \n",
      "79\n",
      "batch: \n",
      "80\n",
      "batch: \n",
      "81\n",
      "batch: \n",
      "82\n",
      "batch: \n",
      "83\n",
      "batch: \n",
      "84\n",
      "batch: \n",
      "85\n",
      "batch: \n",
      "86\n",
      "batch: \n",
      "87\n",
      "batch: \n",
      "88\n",
      "batch: \n",
      "89\n",
      "batch: \n",
      "90\n",
      "batch: \n",
      "91\n",
      "batch: \n",
      "92\n",
      "batch: \n",
      "93\n",
      "batch: \n",
      "94\n",
      "batch: \n",
      "95\n",
      "batch: \n",
      "96\n",
      "batch: \n",
      "97\n",
      "batch: \n",
      "98\n",
      "batch: \n",
      "99\n",
      "batch: \n",
      "100\n",
      "batch: \n",
      "101\n",
      "batch: \n",
      "102\n",
      "batch: \n",
      "103\n",
      "batch: \n",
      "104\n",
      "batch: \n",
      "105\n",
      "batch: \n",
      "106\n",
      "batch: \n",
      "107\n",
      "batch: \n",
      "108\n",
      "batch: \n",
      "109\n",
      "batch: \n",
      "110\n",
      "batch: \n",
      "111\n",
      "batch: \n",
      "112\n",
      "batch: \n",
      "113\n",
      "batch: \n",
      "114\n",
      "batch: \n",
      "115\n",
      "batch: \n",
      "116\n",
      "batch: \n",
      "117\n",
      "batch: \n",
      "118\n",
      "batch: \n",
      "119\n",
      "batch: \n",
      "120\n",
      "batch: \n",
      "121\n",
      "batch: \n",
      "122\n",
      "batch: \n",
      "123\n",
      "batch: \n",
      "124\n",
      "batch: \n",
      "125\n",
      "batch: \n",
      "126\n",
      "batch: \n",
      "127\n",
      "batch: \n",
      "128\n",
      "batch: \n",
      "129\n",
      "batch: \n",
      "130\n",
      "batch: \n",
      "131\n",
      "batch: \n",
      "132\n",
      "batch: \n",
      "133\n",
      "batch: \n",
      "134\n",
      "batch: \n",
      "135\n",
      "batch: \n",
      "136\n",
      "batch: \n",
      "137\n",
      "batch: \n",
      "138\n",
      "batch: \n",
      "139\n",
      "batch: \n",
      "140\n",
      "batch: \n",
      "141\n",
      "batch: \n",
      "142\n",
      "batch: \n",
      "143\n",
      "batch: \n",
      "144\n",
      "batch: \n",
      "145\n",
      "batch: \n",
      "146\n",
      "batch: \n",
      "147\n",
      "batch: \n",
      "148\n",
      "batch: \n",
      "149\n",
      "batch: \n",
      "150\n",
      "batch: \n",
      "151\n",
      "batch: \n",
      "152\n",
      "batch: \n",
      "153\n",
      "batch: \n",
      "154\n",
      "batch: \n",
      "155\n",
      "batch: \n",
      "156\n",
      "batch: \n",
      "157\n",
      "batch: \n",
      "158\n",
      "batch: \n",
      "159\n",
      "batch: \n",
      "160\n",
      "batch: \n",
      "161\n",
      "batch: \n",
      "162\n",
      "batch: \n",
      "163\n",
      "batch: \n",
      "164\n",
      "batch: \n",
      "165\n",
      "batch: \n",
      "166\n",
      "batch: \n",
      "167\n",
      "batch: \n",
      "168\n",
      "batch: \n",
      "169\n",
      "batch: \n",
      "170\n",
      "batch: \n",
      "171\n",
      "batch: \n",
      "172\n",
      "batch: \n",
      "173\n",
      "batch: \n",
      "174\n",
      "batch: \n",
      "175\n",
      "batch: \n",
      "176\n",
      "batch: \n",
      "177\n",
      "batch: \n",
      "178\n",
      "batch: \n",
      "179\n",
      "batch: \n",
      "180\n",
      "batch: \n",
      "181\n",
      "batch: \n",
      "182\n",
      "batch: \n",
      "183\n",
      "batch: \n",
      "184\n",
      "batch: \n",
      "185\n",
      "batch: \n",
      "186\n",
      "batch: \n",
      "187\n",
      "batch: \n",
      "188\n",
      "batch: \n",
      "189\n",
      "batch: \n",
      "190\n",
      "batch: \n",
      "191\n",
      "batch: \n",
      "192\n",
      "batch: \n",
      "193\n",
      "batch: \n",
      "194\n",
      "batch: \n",
      "195\n",
      "batch: \n",
      "196\n",
      "batch: \n",
      "197\n",
      "batch: \n",
      "198\n",
      "batch: \n",
      "199\n",
      "batch: \n",
      "200\n",
      "batch: \n",
      "201\n",
      "batch: \n",
      "202\n",
      "batch: \n",
      "203\n",
      "batch: \n",
      "204\n",
      "batch: \n",
      "205\n",
      "batch: \n",
      "206\n",
      "batch: \n",
      "207\n",
      "batch: \n",
      "208\n",
      "batch: \n",
      "209\n",
      "batch: \n",
      "210\n",
      "train accuracy: \n",
      "0.77977884\n",
      "val accuracy: \n",
      "0.7900474\n",
      "test accuracy: \n",
      "0.76219803\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "def LSTM(x,weights,bias):\n",
    "    embedding_inputs = tf.nn.embedding_lookup(embedding, x) # replace the integers in id list into embedding vectors\n",
    "    lstm_cell=tf.contrib.rnn.BasicLSTMCell(num_units) # generate a LSTM model with num_units cells in each layer\n",
    "    \n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell,embedding_inputs, dtype=tf.float32) # run the model and get the last output\n",
    "    outputs = tf.nn.dropout(outputs, keep_prob)     # dropout process\n",
    "    result = tf.nn.softmax(tf.matmul(outputs[:,-1,:],weights)+bias,1)  # calculate the probability vector\n",
    "\n",
    "    return result\n",
    "\n",
    "prediction=LSTM(x,weights,bias) # get the probability vector\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))  # calculate the cost\n",
    "train_step=tf.train.AdamOptimizer(train_rate).minimize(cost) # use optimizer to improve the model in a given rate\n",
    "correct_predict=tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) # generate accuracy vector\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_predict,tf.float32)) # calculate accuracy\n",
    "\n",
    "init=tf.global_variables_initializer() # initialize all variables\n",
    "\n",
    "# run the training function\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(n_batch):\n",
    "        TrainData_label=y_train[i*batch_size:(i+1)*n_batch]\n",
    "        TrainData_batch = kr.preprocessing.sequence.pad_sequences(X_train.iloc[i*batch_size:(i+1)*n_batch], maxlen) # increase the sequence length to maxlen\n",
    "        sess.run(train_step,feed_dict={x:TrainData_batch,y:TrainData_label, keep_prob: 0.75})\n",
    "        print(\"batch: \")\n",
    "        print(i)\n",
    "    print(\"train accuracy: \")\n",
    "    TrainData = kr.preprocessing.sequence.pad_sequences(X_train, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: TrainData, y: y_train, keep_prob: 1})) # Prediction does not need dropout, so 1 is set\n",
    "    print(\"val accuracy: \")\n",
    "    ValData = kr.preprocessing.sequence.pad_sequences(X_val, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: ValData, y: y_val, keep_prob: 1}))\n",
    "    print(\"test accuracy: \")\n",
    "    TestData = kr.preprocessing.sequence.pad_sequences(X_test, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: TestData, y: y_test, keep_prob: 1}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
