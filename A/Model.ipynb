{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this python, we input the id-list data into an RNN and a LSTM model, train the model and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coated-forth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras as kr\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatty-studio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              text_code      sent\n",
      "0     [1069    2    1 6227    7  159    0  125    5 ...  negative\n",
      "1     [   2   82   27    8   63    0 1654   14  278 ...  negative\n",
      "2     [   0    9    0 3954    0   16 1175   15    0 ...  negative\n",
      "3     [ 300    0    2    0   17    9    0 4333  103 ...  negative\n",
      "4     [   0    0   63    0   52    0    0    6 5692 ...   neutral\n",
      "...                                                 ...       ...\n",
      "5995  [   2    0   75    0   66    0 3415    0  182 ...  positive\n",
      "5996  [ 270   65   91  281    0    0  182   20    0 ...   neutral\n",
      "5997  [  44 1062    0 2784  308  104  280    0    0 ...   neutral\n",
      "5998  [  911    58     0   415     7    11  6207    ...  positive\n",
      "5999  [4377  484    0    0    0  681    0    0  182 ...   neutral\n",
      "\n",
      "[6000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# open and read the id-list\n",
    "df = pd.read_csv('code.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the id-list is a currently in String type, we have to change it into a numeric array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hydraulic-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type-change function\n",
    "def textcode_to_array(textcode):\n",
    "    listOfTokens = re.split(r'\\W+',textcode)\n",
    "    # decrease the length of array since there are spaces at the start and end of the splitted list\n",
    "    codes = np.zeros(len(listOfTokens)-2, dtype=np.int) \n",
    "    for i in range(len(codes)):\n",
    "        codes[i] = int(listOfTokens[i+1])\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "julian-rochester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              text_code      sent  \\\n",
      "0     [1069    2    1 6227    7  159    0  125    5 ...  negative   \n",
      "1     [   2   82   27    8   63    0 1654   14  278 ...  negative   \n",
      "2     [   0    9    0 3954    0   16 1175   15    0 ...  negative   \n",
      "3     [ 300    0    2    0   17    9    0 4333  103 ...  negative   \n",
      "4     [   0    0   63    0   52    0    0    6 5692 ...   neutral   \n",
      "...                                                 ...       ...   \n",
      "5995  [   2    0   75    0   66    0 3415    0  182 ...  positive   \n",
      "5996  [ 270   65   91  281    0    0  182   20    0 ...   neutral   \n",
      "5997  [  44 1062    0 2784  308  104  280    0    0 ...   neutral   \n",
      "5998  [  911    58     0   415     7    11  6207    ...  positive   \n",
      "5999  [4377  484    0    0    0  681    0    0  182 ...   neutral   \n",
      "\n",
      "                                                   code  \n",
      "0     [1069, 2, 1, 6227, 7, 159, 0, 125, 5, 23, 0, 1...  \n",
      "1     [2, 82, 27, 8, 63, 0, 1654, 14, 278, 0, 990, 0...  \n",
      "2     [0, 9, 0, 3954, 0, 16, 1175, 15, 0, 0, 0, 178,...  \n",
      "3             [300, 0, 2, 0, 17, 9, 0, 4333, 103, 0, 2]  \n",
      "4     [0, 0, 63, 0, 52, 0, 0, 6, 5692, 357, 0, 19, 6...  \n",
      "...                                                 ...  \n",
      "5995  [2, 0, 75, 0, 66, 0, 3415, 0, 182, 0, 318, 0, ...  \n",
      "5996  [270, 65, 91, 281, 0, 0, 182, 20, 0, 2459, 15,...  \n",
      "5997  [44, 1062, 0, 2784, 308, 104, 280, 0, 0, 1, 53...  \n",
      "5998  [911, 58, 0, 415, 7, 11, 6207, 0, 10508, 0, 18...  \n",
      "5999  [4377, 484, 0, 0, 0, 681, 0, 0, 182, 0, 53, 0,...  \n",
      "\n",
      "[6000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# do the type change\n",
    "df['code'] = \"\"\n",
    "for i in range(df.shape[0]):\n",
    "    df.iloc[i,2] =  textcode_to_array(df.iloc[i,0])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3 # positive, negative, neutral\n",
    "# use labelencoder to change the type of classes from String to integer\n",
    "le = preprocessing.LabelEncoder() \n",
    "labels = le.fit_transform(df.iloc[:,1])\n",
    "# and then, change the type from integer to one-hot code\n",
    "labels = kr.utils.to_categorical(labels, num_classes) \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "imported-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into train, validation and test sets in a ratio 6:2:2\n",
    "X_trainval,X_test,y_trainval,y_test=train_test_split(df.iloc[:,2],labels,test_size=0.2,random_state=3) \n",
    "X_train,X_val,y_train,y_val=train_test_split(X_trainval,y_trainval,test_size=0.25,random_state=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compliant-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "# find the max length of id-lists\n",
    "maxlen = 0;\n",
    "for i in range(6000):\n",
    "    length = len(df.iloc[i,2])\n",
    "    if length>maxlen:\n",
    "        maxlen = length\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have processed the data, now we should prepare our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 64 # number of cells in a hidden layer\n",
    "batch_size=10 # size of batch\n",
    "n_batch=360 # we have 3600 train data, so 3600/10 = 360 batches\n",
    "train_rate = 0.001 # training rate of optimizer\n",
    "embedding_size = 32 # dimension of embedding set\n",
    "vocabulary_size = 10509 # length of dictionary\n",
    "\n",
    "# placeholders for input data and output probabilities\n",
    "x=tf.placeholder(tf.int32,shape=[None,maxlen]) #\n",
    "y=tf.placeholder(tf.float32,[None,num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases which transform the last output of RNN model into a num_classes dimensional probability vector\n",
    "weights=tf.Variable(tf.truncated_normal([num_units,num_classes],stddev=0.1))\n",
    "biases=tf.Variable(tf.constant(0.1,shape=[num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an embedding matrix for the dictionary\n",
    "embedding = tf.get_variable('embedding', [vocabulary_size, embedding_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probability for dropout process\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef RNN(x,weights,bias):\\n    embedding_inputs = tf.nn.embedding_lookup(embedding, x) # replace the integers in id list into embedding vectors\\n    basic_cell=tf.nn.rnn_cell.BasicRNNCell(num_units) # generate a basic RNN model with num_units cells in each layer\\n    \\n    outputs, states = tf.nn.dynamic_rnn(basic_cell,embedding_inputs, dtype=tf.float32) # run the model and get the last output\\n    outputs = tf.nn.dropout(outputs, keep_prob) # dropout process\\n    result = tf.nn.softmax(tf.matmul(outputs[:,-1,:],weights)+bias,1) # calculate the probability vector\\n    return result\\n\\nprediction=RNN(x,weights,biases) # get the probability vector\\ncost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y)) # calculate the cost\\ntrain_step=tf.train.AdamOptimizer(train_rate).minimize(cost) # use optimizer to improve the model in a given rate\\ncorrect_predict=tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) # generate accuracy vector\\naccuracy=tf.reduce_mean(tf.cast(correct_predict,tf.float32)) # calculate accuracy\\n\\ninit=tf.global_variables_initializer() # initialize all variables\\n\\n# run the training function\\nwith tf.Session() as sess:\\n    sess.run(init)\\n    for i in range(n_batch):\\n        TrainData_label=y_train[i*batch_size:(i+1)*n_batch]\\n        TrainData_batch = kr.preprocessing.sequence.pad_sequences(X_train.iloc[i*batch_size:(i+1)*n_batch], maxlen) # increase the sequence length to maxlen\\n        sess.run(train_step,feed_dict={x:TrainData_batch,y:TrainData_label, keep_prob: 0.75})\\n        print(\"batch: \")\\n        print(i)\\n    print(\"train accuracy: \")\\n    TrainData = kr.preprocessing.sequence.pad_sequences(X_train, maxlen)\\n    print(sess.run(accuracy, feed_dict={x: TrainData, y: y_train, keep_prob: 1})) # Prediction does not need dropout, so 1 is set\\n    print(\"val accuracy: \")\\n    ValData = kr.preprocessing.sequence.pad_sequences(X_val, maxlen)\\n    print(sess.run(accuracy, feed_dict={x: ValData, y: y_val, keep_prob: 1}))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple RNN model\n",
    "'''\n",
    "def RNN(x,weights,bias):\n",
    "    embedding_inputs = tf.nn.embedding_lookup(embedding, x) # replace the integers in id list into embedding vectors\n",
    "    basic_cell=tf.nn.rnn_cell.BasicRNNCell(num_units) # generate a basic RNN model with num_units cells in each layer\n",
    "    \n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell,embedding_inputs, dtype=tf.float32) # run the model and get the last output\n",
    "    outputs = tf.nn.dropout(outputs, keep_prob) # dropout process\n",
    "    result = tf.nn.softmax(tf.matmul(outputs[:,-1,:],weights)+bias,1) # calculate the probability vector\n",
    "    return result\n",
    "\n",
    "prediction=RNN(x,weights,biases) # get the probability vector\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y)) # calculate the cost\n",
    "train_step=tf.train.AdamOptimizer(train_rate).minimize(cost) # use optimizer to improve the model in a given rate\n",
    "correct_predict=tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) # generate accuracy vector\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_predict,tf.float32)) # calculate accuracy\n",
    "\n",
    "init=tf.global_variables_initializer() # initialize all variables\n",
    "\n",
    "# run the training function\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(n_batch):\n",
    "        TrainData_label=y_train[i*batch_size:(i+1)*n_batch]\n",
    "        TrainData_batch = kr.preprocessing.sequence.pad_sequences(X_train.iloc[i*batch_size:(i+1)*n_batch], maxlen) # increase the sequence length to maxlen\n",
    "        sess.run(train_step,feed_dict={x:TrainData_batch,y:TrainData_label, keep_prob: 0.75})\n",
    "        print(\"batch: \")\n",
    "        print(i)\n",
    "    print(\"train accuracy: \")\n",
    "    TrainData = kr.preprocessing.sequence.pad_sequences(X_train, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: TrainData, y: y_train, keep_prob: 1})) # Prediction does not need dropout, so 1 is set\n",
    "    print(\"val accuracy: \")\n",
    "    ValData = kr.preprocessing.sequence.pad_sequences(X_val, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: ValData, y: y_val, keep_prob: 1}))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\lab\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-13-3f64f11989f8>:13: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "batch: \n",
      "0\n",
      "batch: \n",
      "1\n",
      "batch: \n",
      "2\n",
      "batch: \n",
      "3\n",
      "batch: \n",
      "4\n",
      "batch: \n",
      "5\n",
      "batch: \n",
      "6\n",
      "batch: \n",
      "7\n",
      "batch: \n",
      "8\n",
      "batch: \n",
      "9\n",
      "batch: \n",
      "10\n",
      "batch: \n",
      "11\n",
      "batch: \n",
      "12\n",
      "batch: \n",
      "13\n",
      "batch: \n",
      "14\n",
      "batch: \n",
      "15\n",
      "batch: \n",
      "16\n",
      "batch: \n",
      "17\n",
      "batch: \n",
      "18\n",
      "batch: \n",
      "19\n",
      "batch: \n",
      "20\n",
      "batch: \n",
      "21\n",
      "batch: \n",
      "22\n",
      "batch: \n",
      "23\n",
      "batch: \n",
      "24\n",
      "batch: \n",
      "25\n",
      "batch: \n",
      "26\n",
      "batch: \n",
      "27\n",
      "batch: \n",
      "28\n",
      "batch: \n",
      "29\n",
      "batch: \n",
      "30\n",
      "batch: \n",
      "31\n",
      "batch: \n",
      "32\n",
      "batch: \n",
      "33\n",
      "batch: \n",
      "34\n",
      "batch: \n",
      "35\n",
      "batch: \n",
      "36\n",
      "batch: \n",
      "37\n",
      "batch: \n",
      "38\n",
      "batch: \n",
      "39\n",
      "batch: \n",
      "40\n",
      "batch: \n",
      "41\n",
      "batch: \n",
      "42\n",
      "batch: \n",
      "43\n",
      "batch: \n",
      "44\n",
      "batch: \n",
      "45\n",
      "batch: \n",
      "46\n",
      "batch: \n",
      "47\n",
      "batch: \n",
      "48\n",
      "batch: \n",
      "49\n",
      "batch: \n",
      "50\n",
      "batch: \n",
      "51\n",
      "batch: \n",
      "52\n",
      "batch: \n",
      "53\n",
      "batch: \n",
      "54\n",
      "batch: \n",
      "55\n",
      "batch: \n",
      "56\n",
      "batch: \n",
      "57\n",
      "batch: \n",
      "58\n",
      "batch: \n",
      "59\n",
      "batch: \n",
      "60\n",
      "batch: \n",
      "61\n",
      "batch: \n",
      "62\n",
      "batch: \n",
      "63\n",
      "batch: \n",
      "64\n",
      "batch: \n",
      "65\n",
      "batch: \n",
      "66\n",
      "batch: \n",
      "67\n",
      "batch: \n",
      "68\n",
      "batch: \n",
      "69\n",
      "batch: \n",
      "70\n",
      "batch: \n",
      "71\n",
      "batch: \n",
      "72\n",
      "batch: \n",
      "73\n",
      "batch: \n",
      "74\n",
      "batch: \n",
      "75\n",
      "batch: \n",
      "76\n",
      "batch: \n",
      "77\n",
      "batch: \n",
      "78\n",
      "batch: \n",
      "79\n",
      "batch: \n",
      "80\n",
      "batch: \n",
      "81\n",
      "batch: \n",
      "82\n",
      "batch: \n",
      "83\n",
      "batch: \n",
      "84\n",
      "batch: \n",
      "85\n",
      "batch: \n",
      "86\n",
      "batch: \n",
      "87\n",
      "batch: \n",
      "88\n",
      "batch: \n",
      "89\n",
      "batch: \n",
      "90\n",
      "batch: \n",
      "91\n",
      "batch: \n",
      "92\n",
      "batch: \n",
      "93\n",
      "batch: \n",
      "94\n",
      "batch: \n",
      "95\n",
      "batch: \n",
      "96\n",
      "batch: \n",
      "97\n",
      "batch: \n",
      "98\n",
      "batch: \n",
      "99\n",
      "batch: \n",
      "100\n",
      "batch: \n",
      "101\n",
      "batch: \n",
      "102\n",
      "batch: \n",
      "103\n",
      "batch: \n",
      "104\n",
      "batch: \n",
      "105\n",
      "batch: \n",
      "106\n",
      "batch: \n",
      "107\n",
      "batch: \n",
      "108\n",
      "batch: \n",
      "109\n",
      "batch: \n",
      "110\n",
      "batch: \n",
      "111\n",
      "batch: \n",
      "112\n",
      "batch: \n",
      "113\n",
      "batch: \n",
      "114\n",
      "batch: \n",
      "115\n",
      "batch: \n",
      "116\n",
      "batch: \n",
      "117\n",
      "batch: \n",
      "118\n",
      "batch: \n",
      "119\n",
      "batch: \n",
      "120\n",
      "batch: \n",
      "121\n",
      "batch: \n",
      "122\n",
      "batch: \n",
      "123\n",
      "batch: \n",
      "124\n",
      "batch: \n",
      "125\n",
      "batch: \n",
      "126\n",
      "batch: \n",
      "127\n",
      "batch: \n",
      "128\n",
      "batch: \n",
      "129\n",
      "batch: \n",
      "130\n",
      "batch: \n",
      "131\n",
      "batch: \n",
      "132\n",
      "batch: \n",
      "133\n",
      "batch: \n",
      "134\n",
      "batch: \n",
      "135\n",
      "batch: \n",
      "136\n",
      "batch: \n",
      "137\n",
      "batch: \n",
      "138\n",
      "batch: \n",
      "139\n",
      "batch: \n",
      "140\n",
      "batch: \n",
      "141\n",
      "batch: \n",
      "142\n",
      "batch: \n",
      "143\n",
      "batch: \n",
      "144\n",
      "batch: \n",
      "145\n",
      "batch: \n",
      "146\n",
      "batch: \n",
      "147\n",
      "batch: \n",
      "148\n",
      "batch: \n",
      "149\n",
      "batch: \n",
      "150\n",
      "batch: \n",
      "151\n",
      "batch: \n",
      "152\n",
      "batch: \n",
      "153\n",
      "batch: \n",
      "154\n",
      "batch: \n",
      "155\n",
      "batch: \n",
      "156\n",
      "batch: \n",
      "157\n",
      "batch: \n",
      "158\n",
      "batch: \n",
      "159\n",
      "batch: \n",
      "160\n",
      "batch: \n",
      "161\n",
      "batch: \n",
      "162\n",
      "batch: \n",
      "163\n",
      "batch: \n",
      "164\n",
      "batch: \n",
      "165\n",
      "batch: \n",
      "166\n",
      "batch: \n",
      "167\n",
      "batch: \n",
      "168\n",
      "batch: \n",
      "169\n",
      "batch: \n",
      "170\n",
      "batch: \n",
      "171\n",
      "batch: \n",
      "172\n",
      "batch: \n",
      "173\n",
      "batch: \n",
      "174\n",
      "batch: \n",
      "175\n",
      "batch: \n",
      "176\n",
      "batch: \n",
      "177\n",
      "batch: \n",
      "178\n",
      "batch: \n",
      "179\n",
      "batch: \n",
      "180\n",
      "batch: \n",
      "181\n",
      "batch: \n",
      "182\n",
      "batch: \n",
      "183\n",
      "batch: \n",
      "184\n",
      "batch: \n",
      "185\n",
      "batch: \n",
      "186\n",
      "batch: \n",
      "187\n",
      "batch: \n",
      "188\n",
      "batch: \n",
      "189\n",
      "batch: \n",
      "190\n",
      "batch: \n",
      "191\n",
      "batch: \n",
      "192\n",
      "batch: \n",
      "193\n",
      "batch: \n",
      "194\n",
      "batch: \n",
      "195\n",
      "batch: \n",
      "196\n",
      "batch: \n",
      "197\n",
      "batch: \n",
      "198\n",
      "batch: \n",
      "199\n",
      "batch: \n",
      "200\n",
      "batch: \n",
      "201\n",
      "batch: \n",
      "202\n",
      "batch: \n",
      "203\n",
      "batch: \n",
      "204\n",
      "batch: \n",
      "205\n",
      "batch: \n",
      "206\n",
      "batch: \n",
      "207\n",
      "batch: \n",
      "208\n",
      "batch: \n",
      "209\n",
      "batch: \n",
      "210\n",
      "batch: \n",
      "211\n",
      "batch: \n",
      "212\n",
      "batch: \n",
      "213\n",
      "batch: \n",
      "214\n",
      "batch: \n",
      "215\n",
      "batch: \n",
      "216\n",
      "batch: \n",
      "217\n",
      "batch: \n",
      "218\n",
      "batch: \n",
      "219\n",
      "batch: \n",
      "220\n",
      "batch: \n",
      "221\n",
      "batch: \n",
      "222\n",
      "batch: \n",
      "223\n",
      "batch: \n",
      "224\n",
      "batch: \n",
      "225\n",
      "batch: \n",
      "226\n",
      "batch: \n",
      "227\n",
      "batch: \n",
      "228\n",
      "batch: \n",
      "229\n",
      "batch: \n",
      "230\n",
      "batch: \n",
      "231\n",
      "batch: \n",
      "232\n",
      "batch: \n",
      "233\n",
      "batch: \n",
      "234\n",
      "batch: \n",
      "235\n",
      "batch: \n",
      "236\n",
      "batch: \n",
      "237\n",
      "batch: \n",
      "238\n",
      "batch: \n",
      "239\n",
      "batch: \n",
      "240\n",
      "batch: \n",
      "241\n",
      "batch: \n",
      "242\n",
      "batch: \n",
      "243\n",
      "batch: \n",
      "244\n",
      "batch: \n",
      "245\n",
      "batch: \n",
      "246\n",
      "batch: \n",
      "247\n",
      "batch: \n",
      "248\n",
      "batch: \n",
      "249\n",
      "batch: \n",
      "250\n",
      "batch: \n",
      "251\n",
      "batch: \n",
      "252\n",
      "batch: \n",
      "253\n",
      "batch: \n",
      "254\n",
      "batch: \n",
      "255\n",
      "batch: \n",
      "256\n",
      "batch: \n",
      "257\n",
      "batch: \n",
      "258\n",
      "batch: \n",
      "259\n",
      "batch: \n",
      "260\n",
      "batch: \n",
      "261\n",
      "batch: \n",
      "262\n",
      "batch: \n",
      "263\n",
      "batch: \n",
      "264\n",
      "batch: \n",
      "265\n",
      "batch: \n",
      "266\n",
      "batch: \n",
      "267\n",
      "batch: \n",
      "268\n",
      "batch: \n",
      "269\n",
      "batch: \n",
      "270\n",
      "batch: \n",
      "271\n",
      "batch: \n",
      "272\n",
      "batch: \n",
      "273\n",
      "batch: \n",
      "274\n",
      "batch: \n",
      "275\n",
      "batch: \n",
      "276\n",
      "batch: \n",
      "277\n",
      "batch: \n",
      "278\n",
      "batch: \n",
      "279\n",
      "batch: \n",
      "280\n",
      "batch: \n",
      "281\n",
      "batch: \n",
      "282\n",
      "batch: \n",
      "283\n",
      "batch: \n",
      "284\n",
      "batch: \n",
      "285\n",
      "batch: \n",
      "286\n",
      "batch: \n",
      "287\n",
      "batch: \n",
      "288\n",
      "batch: \n",
      "289\n",
      "batch: \n",
      "290\n",
      "batch: \n",
      "291\n",
      "batch: \n",
      "292\n",
      "batch: \n",
      "293\n",
      "batch: \n",
      "294\n",
      "batch: \n",
      "295\n",
      "batch: \n",
      "296\n",
      "batch: \n",
      "297\n",
      "batch: \n",
      "298\n",
      "batch: \n",
      "299\n",
      "batch: \n",
      "300\n",
      "batch: \n",
      "301\n",
      "batch: \n",
      "302\n",
      "batch: \n",
      "303\n",
      "batch: \n",
      "304\n",
      "batch: \n",
      "305\n",
      "batch: \n",
      "306\n",
      "batch: \n",
      "307\n",
      "batch: \n",
      "308\n",
      "batch: \n",
      "309\n",
      "batch: \n",
      "310\n",
      "batch: \n",
      "311\n",
      "batch: \n",
      "312\n",
      "batch: \n",
      "313\n",
      "batch: \n",
      "314\n",
      "batch: \n",
      "315\n",
      "batch: \n",
      "316\n",
      "batch: \n",
      "317\n",
      "batch: \n",
      "318\n",
      "batch: \n",
      "319\n",
      "batch: \n",
      "320\n",
      "batch: \n",
      "321\n",
      "batch: \n",
      "322\n",
      "batch: \n",
      "323\n",
      "batch: \n",
      "324\n",
      "batch: \n",
      "325\n",
      "batch: \n",
      "326\n",
      "batch: \n",
      "327\n",
      "batch: \n",
      "328\n",
      "batch: \n",
      "329\n",
      "batch: \n",
      "330\n",
      "batch: \n",
      "331\n",
      "batch: \n",
      "332\n",
      "batch: \n",
      "333\n",
      "batch: \n",
      "334\n",
      "batch: \n",
      "335\n",
      "batch: \n",
      "336\n",
      "batch: \n",
      "337\n",
      "batch: \n",
      "338\n",
      "batch: \n",
      "339\n",
      "batch: \n",
      "340\n",
      "batch: \n",
      "341\n",
      "batch: \n",
      "342\n",
      "batch: \n",
      "343\n",
      "batch: \n",
      "344\n",
      "batch: \n",
      "345\n",
      "batch: \n",
      "346\n",
      "batch: \n",
      "347\n",
      "batch: \n",
      "348\n",
      "batch: \n",
      "349\n",
      "batch: \n",
      "350\n",
      "batch: \n",
      "351\n",
      "batch: \n",
      "352\n",
      "batch: \n",
      "353\n",
      "batch: \n",
      "354\n",
      "batch: \n",
      "355\n",
      "batch: \n",
      "356\n",
      "batch: \n",
      "357\n",
      "batch: \n",
      "358\n",
      "batch: \n",
      "359\n",
      "train accuracy: \n",
      "0.76611114\n",
      "val accuracy: \n",
      "0.4475\n",
      "test accuracy: \n",
      "0.46083334\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "def LSTM(x,weights,bias):\n",
    "    embedding_inputs = tf.nn.embedding_lookup(embedding, x) # replace the integers in id list into embedding vectors\n",
    "    lstm_cell=tf.contrib.rnn.BasicLSTMCell(num_units) # generate a LSTM model with num_units cells in each layer\n",
    "    \n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell,embedding_inputs, dtype=tf.float32) # run the model and get the last output\n",
    "    outputs = tf.nn.dropout(outputs, keep_prob)     # dropout process\n",
    "    result = tf.nn.softmax(tf.matmul(outputs[:,-1,:],weights)+bias,1)  # calculate the probability vector\n",
    "\n",
    "    return result\n",
    "\n",
    "prediction=LSTM(x,weights,biases) # get the probability vector\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))  # calculate the cost\n",
    "train_step=tf.train.AdamOptimizer(train_rate).minimize(cost) # use optimizer to improve the model in a given rate\n",
    "correct_predict=tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) # generate accuracy vector\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_predict,tf.float32)) # calculate accuracy\n",
    "\n",
    "init=tf.global_variables_initializer() # initialize all variables\n",
    "\n",
    "# run the training function\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(n_batch):\n",
    "        TrainData_label=y_train[i*batch_size:(i+1)*n_batch]\n",
    "        TrainData_batch = kr.preprocessing.sequence.pad_sequences(X_train.iloc[i*batch_size:(i+1)*n_batch], maxlen) # increase the sequence length to maxlen\n",
    "        sess.run(train_step,feed_dict={x:TrainData_batch,y:TrainData_label, keep_prob: 0.75})\n",
    "        print(\"batch: \")\n",
    "        print(i)\n",
    "    print(\"train accuracy: \")\n",
    "    TrainData = kr.preprocessing.sequence.pad_sequences(X_train, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: TrainData, y: y_train, keep_prob: 1})) # Prediction does not need dropout, so 1 is set\n",
    "    print(\"val accuracy: \")\n",
    "    ValData = kr.preprocessing.sequence.pad_sequences(X_val, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: ValData, y: y_val, keep_prob: 1}))\n",
    "    print(\"test accuracy: \")\n",
    "    TestData = kr.preprocessing.sequence.pad_sequences(X_test, maxlen)\n",
    "    print(sess.run(accuracy, feed_dict={x: TestData, y: y_test, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
